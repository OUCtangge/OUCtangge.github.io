{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "0b0fG_uXtQr2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIW-If53CiPL"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czOvyn7HCbgV"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/google-research/swirl-dynamics.git@main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2DJBuEdCpFc"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP6GQNwnCrwz"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDKhSAGaCrk2"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "from clu import metric_writers\n",
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from swirl_dynamics import templates\n",
        "from swirl_dynamics.lib import diffusion as dfn_lib\n",
        "from swirl_dynamics.lib import solvers as solver_lib\n",
        "from swirl_dynamics.projects import probabilistic_diffusion as dfn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ylBqVdcCvcz"
      },
      "source": [
        "## Example I - Unconditional diffusion model with guidance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVzRTpB5Dgm2"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THwrp-2UD2iS"
      },
      "source": [
        "First we need a dataset containing samples whose distribution is to be modeled by the diffusion model. For demonstration purpose, we use the MNIST dataset provided by TensorFlow Datasets.\n",
        "\n",
        "Our code setup accepts any Python iterable objects to be used as dataloaders. The expectation is that they should continuously yield a dictionary with a field named `x` whose corresponding value is a numpy array with shape `(batch, *spatial_dims, channels)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRf3AcadCvKj"
      },
      "outputs": [],
      "source": [
        "def get_mnist_dataset(split: str, batch_size: int):\n",
        "  ds = tfds.load(\"mnist\", split=split)\n",
        "  ds = ds.map(\n",
        "      # Change field name from \"image\" to \"x\" (required by `DenoisingModel`)\n",
        "      # and normalize the value to [0, 1].\n",
        "      lambda x: {\"x\": tf.cast(x[\"image\"], tf.float32) / 255.0}\n",
        "  )\n",
        "  ds = ds.repeat()\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "  ds = ds.as_numpy_iterator()\n",
        "  return ds\n",
        "\n",
        "# The standard deviation of the normalized dataset.\n",
        "# This is useful for determining the diffusion scheme and preconditioning\n",
        "# of the neural network parametrization.\n",
        "DATA_STD = 0.31"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5tZdk5eEQhh"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA4hHUeHEVoE"
      },
      "source": [
        "Next let's define the U-Net backbone. The \"Preconditioning\" is to ensure that the inputs and outputs of the network are roughly standardized (for more details, see Appendix B.6. in [this paper](https://arxiv.org/abs/2206.00364))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE1-wf7aETEH"
      },
      "outputs": [],
      "source": [
        "denoiser_model = dfn_lib.PreconditionedDenoiserUNet(\n",
        "    out_channels=1,\n",
        "    num_channels=(64, 128),\n",
        "    downsample_ratio=(2, 2),\n",
        "    num_blocks=4,\n",
        "    noise_embed_dim=128,\n",
        "    padding=\"SAME\",\n",
        "    use_attention=True,\n",
        "    use_position_encoding=True,\n",
        "    num_heads=8,\n",
        "    sigma_data=DATA_STD,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htjx7TxAEsKW"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5i7oh2WFFU0"
      },
      "source": [
        "For diffusion model training, the above-defined U-Net backbone serves as a denoiser, which takes as input a batch of (isotropic Gaussian noise) corrupted samples and outputs its best guess for what the uncorrupted image would be.\n",
        "\n",
        "Besides the backbone architecture, we also need to specify how to sample the noise levels (i.e. standard deviations) used to corrupt the samples and the weighting for each noise level in the loss function (for available options and configurations, see [`swirl_dynamics.lib.diffusion.diffusion`](https://github.com/google-research/swirl-dynamics/blob/main/swirl_dynamics/lib/diffusion/diffusion.py)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l52TFsPEEp5u"
      },
      "outputs": [],
      "source": [
        "diffusion_scheme = dfn_lib.Diffusion.create_variance_exploding(\n",
        "    sigma=dfn_lib.tangent_noise_schedule(),\n",
        "    data_std=DATA_STD,\n",
        ")\n",
        "\n",
        "model = dfn.DenoisingModel(\n",
        "    # `input_shape` must agree with the expected sample shape (without the batch\n",
        "    # dimension), which in this case is simply the dimensions of a single MNIST\n",
        "    # sample.\n",
        "    input_shape=(28, 28, 1),\n",
        "    denoiser=denoiser_model,\n",
        "    noise_sampling=dfn_lib.log_uniform_sampling(\n",
        "        diffusion_scheme, clip_min=1e-4, uniform_grid=True,\n",
        "    ),\n",
        "    noise_weighting=dfn_lib.edm_weighting(data_std=DATA_STD),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76a8RhUpFLoe"
      },
      "source": [
        "We are now ready to define the learning parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8fl_R1gEp36"
      },
      "outputs": [],
      "source": [
        "# !rm -R -f $workdir  # optional: clear the working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4DAOL_xEp1k"
      },
      "outputs": [],
      "source": [
        "num_train_steps = 100_000  #@param\n",
        "workdir = \"/tmp/diffusion_demo_mnist\"  #@param\n",
        "train_batch_size = 32  #@param\n",
        "eval_batch_size = 32  #@param\n",
        "initial_lr = 0.0  #@param\n",
        "peak_lr = 1e-4  #@param\n",
        "warmup_steps = 1000  #@param\n",
        "end_lr = 1e-6  #@param\n",
        "ema_decay = 0.999  #@param\n",
        "ckpt_interval = 1000  #@param\n",
        "max_ckpt_to_keep = 5  #@param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr-be7CiFRhp"
      },
      "source": [
        "To start training, we first need to initialize the trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7E4WZUFEpzv"
      },
      "outputs": [],
      "source": [
        "# NOTE: use `trainers.DistributedDenoisingTrainer` for multi-device\n",
        "# training with data parallelism.\n",
        "trainer = dfn.DenoisingTrainer(\n",
        "    model=model,\n",
        "    rng=jax.random.PRNGKey(888),\n",
        "    optimizer=optax.adam(\n",
        "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
        "            init_value=initial_lr,\n",
        "            peak_value=peak_lr,\n",
        "            warmup_steps=warmup_steps,\n",
        "            decay_steps=num_train_steps,\n",
        "            end_value=end_lr,\n",
        "        ),\n",
        "    ),\n",
        "    # We keep track of an exponential moving average of the model parameters\n",
        "    # over training steps. This alleviates the \"color-shift\" problems known to\n",
        "    # exist in the diffusion models.\n",
        "    ema_decay=ema_decay,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTQf9w1NFgp7"
      },
      "source": [
        "Now we are ready to kick start training. A couple of \"callbacks\" are passed to assist with monitoring and checkpointing.\n",
        "\n",
        "The first step will be a little slow as Jax needs to JIT compile the step function (the same goes for the first step where evaluation is performed). Fortunately, steps after that should continue much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMQbydfdEpxQ"
      },
      "outputs": [],
      "source": [
        "templates.run_train(\n",
        "    train_dataloader=get_mnist_dataset(\n",
        "        split=\"train[:75%]\", batch_size=train_batch_size\n",
        "    ),\n",
        "    trainer=trainer,\n",
        "    workdir=workdir,\n",
        "    total_train_steps=num_train_steps,\n",
        "    metric_writer=metric_writers.create_default_writer(\n",
        "        workdir, asynchronous=False\n",
        "    ),\n",
        "    metric_aggregation_steps=100,\n",
        "    eval_dataloader=get_mnist_dataset(\n",
        "        split=\"train[75%:]\", batch_size=eval_batch_size\n",
        "    ),\n",
        "    eval_every_steps = 1000,\n",
        "    num_batches_per_eval = 2,\n",
        "    callbacks=(\n",
        "        # This callback displays the training progress in a tqdm bar\n",
        "        templates.TqdmProgressBar(\n",
        "            total_train_steps=num_train_steps,\n",
        "            train_monitors=(\"train_loss\",),\n",
        "        ),\n",
        "        # This callback saves model checkpoint periodically\n",
        "        templates.TrainStateCheckpoint(\n",
        "            base_dir=workdir,\n",
        "            options=ocp.CheckpointManagerOptions(\n",
        "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
        "            ),\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJDsoPMcFnJ0"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGPHjX8bFqrX"
      },
      "source": [
        "#### Unconditional generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUHa793NF0AS"
      },
      "source": [
        "After training is complete, the trained denoiser may be used to generate unconditional samples.\n",
        "\n",
        "First, let's restore the model from checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-CbKOV9EpvI"
      },
      "outputs": [],
      "source": [
        "# Restore train state from checkpoint. By default, the move recently saved\n",
        "# checkpoint is restored. Alternatively, one can directly use\n",
        "# `trainer.train_state` if continuing from the training section above.\n",
        "trained_state = dfn.DenoisingModelTrainState.restore_from_orbax_ckpt(\n",
        "    f\"{workdir}/checkpoints\", step=None\n",
        ")\n",
        "# Construct the inference function\n",
        "denoise_fn = dfn.DenoisingTrainer.inference_fn_from_state_dict(\n",
        "    trained_state, use_ema=True, denoiser=denoiser_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT2c5b6FF9jP"
      },
      "source": [
        "Diffusion samples are generated by plugging the trained denoising function in a stochastic differential equation (parametrized by the diffusion scheme) and solving it backwards in time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiOByVtjEpUs"
      },
      "outputs": [],
      "source": [
        "sampler = dfn_lib.SdeSampler(\n",
        "    input_shape=(28, 28, 1),\n",
        "    integrator=solver_lib.EulerMaruyama(),\n",
        "    tspan=dfn_lib.edm_noise_decay(\n",
        "        diffusion_scheme, rho=7, num_steps=256, end_sigma=1e-3,\n",
        "    ),\n",
        "    scheme=diffusion_scheme,\n",
        "    denoise_fn=denoise_fn,\n",
        "    guidance_transforms=(),\n",
        "    apply_denoise_at_end=True,\n",
        "    return_full_paths=False,  # Set to `True` if the full sampling paths are needed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA26cPp0GBwC"
      },
      "source": [
        "The sampler may be run by calling its `.generate()` function. Optionally, we may JIT compile this function so that it runs faster if repeatedly called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7yaiI7NF_61"
      },
      "outputs": [],
      "source": [
        "generate = jax.jit(sampler.generate, static_argnames=('num_samples',))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qUxgUaKGENj"
      },
      "outputs": [],
      "source": [
        "samples = generate(\n",
        "    rng=jax.random.PRNGKey(8888), num_samples=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTjpNNHtGH9c"
      },
      "source": [
        "Visualize the generated samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thhRfIe5GELQ"
      },
      "outputs": [],
      "source": [
        "# Plot generated samples\n",
        "fig, ax = plt.subplots(1, 4, figsize=(8, 2))\n",
        "for i in range(4):\n",
        "  im = ax[i].imshow(samples[i, :, :, 0] * 255, cmap=\"gray\", vmin=0, vmax=255)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cr6J3hfGLi7"
      },
      "source": [
        "#### Guided generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8-3Hr5IGReH"
      },
      "source": [
        "To achieve 'guided' generation, we can modify a trained denoising function and tailor it to produce samples with specific desired characteristics. For instance, in an out-filling task where the goal is to generate full images from a given patch, we can guide the denoiser to create samples whose crops at certain positions precisely align with the provided patch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7P4li0qGEI4"
      },
      "outputs": [],
      "source": [
        "guidance_fn = dfn_lib.InfillFromSlices(\n",
        "    # This specifies location of the guide input using python slices.\n",
        "    # Here it implies that the guide input corresponds the 7x7 patch in the\n",
        "    # center of the image.\n",
        "    slices=(slice(None), slice(11, 18), slice(11, 18)),\n",
        "\n",
        "    # This is a parameter that controls how \"hard\" the denoiser pushes for\n",
        "    # the conditioning to be satisfied. It is a tradeoff between strictness of\n",
        "    # constraint satisfication and diversity in the generated samples.\n",
        "    guide_strength=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D5zBynzGUdY"
      },
      "source": [
        "This transform function is passed through the `guidance_transforms` arg of the sampler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_kJnXqjGEGz"
      },
      "outputs": [],
      "source": [
        "guided_sampler = dfn_lib.SdeSampler(\n",
        "    input_shape=(28, 28, 1),\n",
        "    integrator=solver_lib.EulerMaruyama(),\n",
        "    tspan=dfn_lib.edm_noise_decay(\n",
        "        diffusion_scheme, rho=7, num_steps=256, end_sigma=1e-3,\n",
        "    ),\n",
        "    scheme=diffusion_scheme,\n",
        "    denoise_fn=denoise_fn,\n",
        "    guidance_transforms=(guidance_fn,),\n",
        "    apply_denoise_at_end=True,\n",
        "    return_full_paths=False,\n",
        ")\n",
        "\n",
        "guided_generate = jax.jit(guided_sampler.generate, static_argnames=('num_samples',))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcDPFQ9PGX9-"
      },
      "source": [
        "We construct an example guidance input from a real sample and use it to guide the sampling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5Uap93nGEEx"
      },
      "outputs": [],
      "source": [
        "test_ds = get_mnist_dataset(split=\"test\", batch_size=1)\n",
        "test_example = next(iter(test_ds))[\"x\"]\n",
        "example_guidance_inputs = {'observed_slices': test_example[:, 11:18, 11:18]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdUrseQOGECo"
      },
      "outputs": [],
      "source": [
        "guided_samples = guided_generate(\n",
        "    rng=jax.random.PRNGKey(66),\n",
        "    num_samples=4,\n",
        "    # Note that the shape of the guidance input must be compatible with\n",
        "    # `sample[guidance_fn.slices]`\n",
        "    guidance_inputs=example_guidance_inputs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auf-UA3yGew-"
      },
      "source": [
        "Visualize guided samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72jBzmTCGEAg"
      },
      "outputs": [],
      "source": [
        "# Plot guide patch.\n",
        "fig, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
        "im = ax.imshow(\n",
        "    test_example[0, 11:18, 11:18, 0] * 255, cmap=\"gray\", vmin=0, vmax=255\n",
        ")\n",
        "ax.axis(\"off\")\n",
        "ax.set_title(\"Guide patch\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot generated samples.\n",
        "fig, ax = plt.subplots(1, 4, figsize=(8, 2))\n",
        "for i in range(4):\n",
        "  im = ax[i].imshow(\n",
        "      guided_samples[i, :, :, 0] * 255, cmap=\"gray\", vmin=0, vmax=255\n",
        "  )\n",
        "  # Mark out the patch where guidance is enabled.\n",
        "  square = patches.Rectangle(\n",
        "      xy=(11, 11), width=7, height=7, fill=False, edgecolor='red'\n",
        "  )\n",
        "  ax[i].add_patch(square)\n",
        "  ax[i].axis(\"off\")\n",
        "  ax[i].set_title(f\"Sample #{i}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq4xz1w9GkFE"
      },
      "source": [
        "## Example II - Conditional diffusion model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElgwVoIPGxRq"
      },
      "source": [
        "In the above example, we trained an *unconditional* diffusion model and applied conditioning at inference time. This is not always easy to do, depending on how the conditioning input relates to the samples.\n",
        "\n",
        "Alternatively, we can directly *train a conditional model*, where the conditional signal is provided at training time as an additional input to the denoising neural network, which may then use it to compute the denoised target.\n",
        "\n",
        "Below we show an example of how to accomplish this. We again generate samples of handwritten digits, using the MNIST dataset for training. We will condition the generation on the `x[11:18, 11:18]` patch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U-O2msbGzEx"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba3Wn2YvG1oC"
      },
      "source": [
        "Besides the sample in `x`, the dataset for training conditional models require a `cond` key which contains the condition signals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IpRYEJtGD-Q"
      },
      "outputs": [],
      "source": [
        "def preproc_example(example: dict[str, tf.Tensor]):\n",
        "  processed = {}\n",
        "  processed[\"x\"] = tf.cast(example[\"image\"], tf.float32) / 255.0\n",
        "\n",
        "  # The \"channel:\" prefix indicate that the conditioning signal is to be\n",
        "  # incorporated by resizing and concatenating along the channel dimension.\n",
        "  # This is implemented at the backbone level.\n",
        "  processed[\"cond\"] = {\"channel:low_res\": processed[\"x\"][11:18, 11:18]}\n",
        "  return processed\n",
        "\n",
        "\n",
        "def get_cond_mnist_dataset(split: str, batch_size: int):\n",
        "  ds = tfds.load(\"mnist\", split=split)\n",
        "  ds = ds.map(preproc_example)\n",
        "  ds = ds.repeat()\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "  ds = ds.as_numpy_iterator()\n",
        "  return ds\n",
        "\n",
        "DATA_STD = 0.31"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yOBMiJtG7r3"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZNUY5kQG9xd"
      },
      "source": [
        "The architecture is similar to the unconditional case. We provide additional args that specify how to resize the conditioning signal (in order to be compatible with the noisy sample for channel-wise concatenation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5F8kNMAGiTR"
      },
      "outputs": [],
      "source": [
        "cond_denoiser_model = dfn_lib.PreconditionedDenoiserUNet(\n",
        "    out_channels=1,\n",
        "    num_channels=(64, 128),\n",
        "    downsample_ratio=(2, 2),\n",
        "    num_blocks=4,\n",
        "    noise_embed_dim=128,\n",
        "    padding=\"SAME\",\n",
        "    use_attention=True,\n",
        "    use_position_encoding=True,\n",
        "    num_heads=8,\n",
        "    sigma_data=DATA_STD,\n",
        "    cond_resize_method=\"cubic\",\n",
        "    cond_embed_dim=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19oJrFsjHCIZ"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT0JP9yAHEem"
      },
      "source": [
        "The `DenoisingModel` is again similar to the unconditional case. We additionally provide the shape information of the `cond` input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJFKb060GiRH"
      },
      "outputs": [],
      "source": [
        "diffusion_scheme = dfn_lib.Diffusion.create_variance_exploding(\n",
        "    sigma=dfn_lib.tangent_noise_schedule(),\n",
        "    data_std=DATA_STD,\n",
        ")\n",
        "\n",
        "cond_model = dfn.DenoisingModel(\n",
        "    input_shape=(28, 28, 1),\n",
        "    # `cond_shape` must agree with the expected structure and shape\n",
        "    # (without the batch dimension) of the `cond` input.\n",
        "    cond_shape={\"channel:low_res\": (7, 7, 1)},\n",
        "    denoiser=cond_denoiser_model,\n",
        "    noise_sampling=dfn_lib.log_uniform_sampling(\n",
        "        diffusion_scheme, clip_min=1e-4, uniform_grid=True,\n",
        "    ),\n",
        "    noise_weighting=dfn_lib.edm_weighting(data_std=DATA_STD),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81Bktr9gHHjz"
      },
      "source": [
        "The rest mostly repeats the unconditional training example, replacing the datasets and model with their conditional counterparts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyEsCCSbGiPC"
      },
      "outputs": [],
      "source": [
        "# !rm -R -f $cond_workdir  # optional: clear the working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekXD8PprGiM8"
      },
      "outputs": [],
      "source": [
        "num_train_steps = 100_000  #@param\n",
        "cond_workdir = \"/tmp/cond_diffusion_demo_mnist\"  #@param\n",
        "train_batch_size = 32  #@param\n",
        "eval_batch_size = 32  #@param\n",
        "initial_lr = 0.0  #@param\n",
        "peak_lr = 1e-4  #@param\n",
        "warmup_steps = 1000  #@param\n",
        "end_lr = 1e-6  #@param\n",
        "ema_decay = 0.999  #@param\n",
        "ckpt_interval = 1000  #@param\n",
        "max_ckpt_to_keep = 5  #@param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DDpmV-zGiKW"
      },
      "outputs": [],
      "source": [
        "cond_trainer = dfn.DenoisingTrainer(\n",
        "    model=cond_model,\n",
        "    rng=jax.random.PRNGKey(888),\n",
        "    optimizer=optax.adam(\n",
        "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
        "            init_value=initial_lr,\n",
        "            peak_value=peak_lr,\n",
        "            warmup_steps=warmup_steps,\n",
        "            decay_steps=num_train_steps,\n",
        "            end_value=end_lr,\n",
        "        ),\n",
        "    ),\n",
        "    ema_decay=ema_decay,\n",
        ")\n",
        "\n",
        "templates.run_train(\n",
        "    train_dataloader=get_cond_mnist_dataset(\n",
        "        split=\"train[:75%]\", batch_size=train_batch_size\n",
        "    ),\n",
        "    trainer=cond_trainer,\n",
        "    workdir=cond_workdir,\n",
        "    total_train_steps=num_train_steps,\n",
        "    metric_writer=metric_writers.create_default_writer(\n",
        "        cond_workdir, asynchronous=False\n",
        "    ),\n",
        "    metric_aggregation_steps=100,\n",
        "    eval_dataloader=get_cond_mnist_dataset(\n",
        "        split=\"train[75%:]\", batch_size=eval_batch_size\n",
        "    ),\n",
        "    eval_every_steps = 1000,\n",
        "    num_batches_per_eval = 2,\n",
        "    callbacks=(\n",
        "        templates.TqdmProgressBar(\n",
        "            total_train_steps=num_train_steps,\n",
        "            train_monitors=(\"train_loss\",),\n",
        "        ),\n",
        "        templates.TrainStateCheckpoint(\n",
        "            base_dir=cond_workdir,\n",
        "            options=ocp.CheckpointManagerOptions(\n",
        "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
        "            ),\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojUo2JDEHPCN"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS0m_f0CHR5i"
      },
      "source": [
        "To perform inference/sampling, let's load back the trained conditional model checkpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RHlke6pGiHx"
      },
      "outputs": [],
      "source": [
        "trained_state = dfn.DenoisingModelTrainState.restore_from_orbax_ckpt(\n",
        "    f\"{cond_workdir}/checkpoints\", step=None\n",
        ")\n",
        "# Construct the inference function\n",
        "cond_denoise_fn = dfn.DenoisingTrainer.inference_fn_from_state_dict(\n",
        "    trained_state, use_ema=True, denoiser=cond_denoiser_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3crKP7hqHV7I"
      },
      "source": [
        "The conditional sampler again follows the previous example, with the only exception being that the conditional model replaces the unconditional one.\n",
        "\n",
        "Below we do not apply any guidance, but one can be easily added in the same way as in the unconditional example above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnaFzOjOHOu4"
      },
      "outputs": [],
      "source": [
        "cond_sampler = dfn_lib.SdeSampler(\n",
        "    input_shape=(28, 28, 1),\n",
        "    integrator=solver_lib.EulerMaruyama(),\n",
        "    tspan=dfn_lib.edm_noise_decay(\n",
        "        diffusion_scheme, rho=7, num_steps=256, end_sigma=1e-3,\n",
        "    ),\n",
        "    scheme=diffusion_scheme,\n",
        "    denoise_fn=cond_denoise_fn,\n",
        "    guidance_transforms=(),\n",
        "    apply_denoise_at_end=True,\n",
        "    return_full_paths=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lYF1OUEHZFM"
      },
      "source": [
        "We again JIT the generate function for the sake of faster repeated sampling calls. Here we employ `functools.partial` to specify `num_samples=5`, making it easier to vectorize across the batch dimension with `jax.vmap`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT_rLzdgHOsm"
      },
      "outputs": [],
      "source": [
        "num_samples_per_cond = 5\n",
        "\n",
        "generate = jax.jit(\n",
        "    functools.partial(cond_sampler.generate, num_samples_per_cond)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_9TfCMSHd3P"
      },
      "source": [
        "Loading a test batch of conditions with 4 elements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tODWrPBfHOqN"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "test_ds = get_cond_mnist_dataset(split=\"test\", batch_size=4)\n",
        "test_batch_cond = next(iter(test_ds))[\"cond\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HJXzAtzHhli"
      },
      "source": [
        "The vectorized generate function is applied to the loaded batch. The vectorization occurs for the leading dimensions of both the random seed and the condition (for those unfamiliarized with vectorized operations in jax, think of a more efficient `for` loop that iterates over the random seeds and batch conditions zipped together)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29oPpGuWHhYP"
      },
      "outputs": [],
      "source": [
        "cond_samples = jax.vmap(generate, in_axes=(0, 0, None))(\n",
        "    jax.random.split(jax.random.PRNGKey(8888), batch_size),\n",
        "    test_batch_cond,\n",
        "    None,  # Guidance inputs = None since no guidance transforms involved\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH9CozY8Hkgv"
      },
      "source": [
        "The result `cond_samples` has shape `(batch_size, num_samples_per_cond, *input_shape)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSQHV5FmHOoK"
      },
      "outputs": [],
      "source": [
        "print(cond_samples.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br6clK2cHow9"
      },
      "source": [
        "Visualize generated examples alongside their low-res conditioning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7c5wXDcHOkP"
      },
      "outputs": [],
      "source": [
        "for i in range(batch_size):\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
        "  im = ax.imshow(\n",
        "      test_batch_cond[\"channel:low_res\"][i, :, :, 0] * 255,\n",
        "      cmap=\"gray\", vmin=0, vmax=255\n",
        "  )\n",
        "  ax.axis(\"off\")\n",
        "  ax.set_title(f\"Low-res condition: #{i + 1}\")\n",
        "\n",
        "\n",
        "  # Plot generated samples.\n",
        "  fig, ax = plt.subplots(\n",
        "      1, num_samples_per_cond, figsize=(num_samples_per_cond * 2, 2)\n",
        "  )\n",
        "  for j in range(num_samples_per_cond):\n",
        "    im = ax[j].imshow(\n",
        "        cond_samples[i, j, :, :, 0] * 255, cmap=\"gray\", vmin=0, vmax=255\n",
        "    )\n",
        "    square = patches.Rectangle(\n",
        "        xy=(11, 11), width=7, height=7, fill=False, edgecolor='red'\n",
        "    )\n",
        "    ax[j].add_patch(square)\n",
        "    ax[j].set_title(f\"conditional sample: #{j + 1}\")\n",
        "    ax[j].axis(\"off\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "new内容\n"
      ],
      "metadata": {
        "id": "cTrW0GAgtT_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchdiffeq import odeint_adjoint as odeint  # 用于SDE/SDDE积分\n",
        "\n",
        "\n",
        "# ============= 1. 基础组件：现代Hopfield层（三种模型复用） =============\n",
        "class HopfieldLayer(nn.Module):\n",
        "    \"\"\"基于连续状态的现代Hopfield网络（参考2008.02217v3.pdf的连续记忆机制）\"\"\"\n",
        "    def __init__(self, dim_model, num_heads=4, beta=1.0):\n",
        "        super().__init__()\n",
        "        self.dim_model = dim_model\n",
        "        self.num_heads = num_heads\n",
        "        self.beta = beta  # 温度系数，调节注意力权重锐度\n",
        "        self.head_dim = dim_model // num_heads\n",
        "        assert dim_model % num_heads == 0, \"dim_model must be divisible by num_heads\"\n",
        "\n",
        "        # Query/Key/Value投影（实现模式存储与检索，参考文档的Key-Query映射）\n",
        "        self.query_proj = nn.Linear(dim_model, dim_model)\n",
        "        self.key_proj = nn.Linear(dim_model, dim_model)\n",
        "        self.value_proj = nn.Linear(dim_model, dim_model)\n",
        "        self.output_proj = nn.Linear(dim_model, dim_model)  # 残差投影\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x: (batch_size, seq_len, dim_model)，输出带记忆增强的特征\"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # 1. 线性投影：将输入映射到Hopfield空间\n",
        "        Q = self.query_proj(x)  # (B, T, D)\n",
        "        K = self.key_proj(x)    # (B, T, D)\n",
        "        V = self.value_proj(x)  # (B, T, D)\n",
        "\n",
        "        # 2. 多头注意力拆分（并行记忆检索，提升模式匹配精度）\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (B, H, T, hD)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # 3. Hopfield记忆更新：softmax注意力近似模式关联（参考文档的ξ_new = X·softmax(βX^Tξ)）\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)  # 相似度分数\n",
        "        scores = scores * self.beta  # 温度调节\n",
        "        attn_weights = F.softmax(scores, dim=-1)  # 记忆关联权重\n",
        "\n",
        "        # 4. 加权检索记忆模式\n",
        "        memory_output = torch.matmul(attn_weights, V)  # (B, H, T, hD)\n",
        "\n",
        "        # 5. 合并多头并残差连接（保留原始特征，避免记忆偏移）\n",
        "        memory_output = memory_output.transpose(1, 2).contiguous()  # (B, T, H, hD)\n",
        "        memory_output = memory_output.view(batch_size, seq_len, self.dim_model)  # (B, T, D)\n",
        "        memory_output = self.output_proj(memory_output) + x  # 残差连接\n",
        "\n",
        "        return memory_output\n",
        "\n",
        "\n",
        "# ============= 2. 对比模型1：神经随机时滞微分方程（SDDE-Net） =============\n",
        "class SDDE_Net(nn.Module):\n",
        "    \"\"\"神经随机时滞微分方程模型（参考SDDE_net.pdf，含时滞项建模）\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=32, tau=2):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tau = tau  # 时滞步数：使用前tau步的状态作为延迟输入（核心区别于SDE）\n",
        "\n",
        "        # 1. 时滞特征编码器：拼接当前+延迟状态，建模时滞依赖\n",
        "        self.lag_encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim * (tau + 1), hidden_dim),  # 输入：当前状态+前tau步状态\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # 2. SDDE核心组件：漂移项（确定性时滞动力学）+ 扩散项（随机时滞噪声）\n",
        "        self.drift_net = nn.Sequential(  # 漂移项：建模带时滞的确定性动态\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        self.diffusion_net = nn.Sequential(  # 扩散项：建模带时滞的随机性强度\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # 3. 解码器：从SDDE隐藏空间映射回原始输出\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "\n",
        "    def get_lag_state(self, x_seq):\n",
        "        \"\"\"提取时滞状态：x_seq (B, T, D) → 拼接当前状态+前tau步状态 (B, T-tau, D*(tau+1))\"\"\"\n",
        "        batch_size, seq_len, _ = x_seq.shape\n",
        "        lag_states = []\n",
        "        # 从第tau步开始，每步拼接前tau步状态\n",
        "        for t in range(self.tau, seq_len):\n",
        "            lag_window = x_seq[:, t - self.tau : t + 1, :]  # (B, tau+1, D)\n",
        "            lag_window_flat = lag_window.view(batch_size, -1)  # (B, D*(tau+1))\n",
        "            lag_states.append(lag_window_flat)\n",
        "        return torch.stack(lag_states, dim=1)  # (B, T-tau, D*(tau+1))\n",
        "\n",
        "    def sde_dynamics(self, t, z):\n",
        "        \"\"\"SDDE动力学方程：dz = drift(t,z)dt + diffusion(t,z)dW(t)\"\"\"\n",
        "        drift = self.drift_net(z)  # 漂移项：带时滞的确定性增量\n",
        "        diffusion = F.softplus(self.diffusion_net(z)) + 1e-6  # 扩散项：确保非负噪声强度\n",
        "        return drift, diffusion\n",
        "\n",
        "    def forward(self, x_seq, t_eval=None):\n",
        "        batch_size, seq_len, _ = x_seq.shape\n",
        "        device = x_seq.device\n",
        "\n",
        "        # 1. 提取时滞特征（核心：建模时滞依赖）\n",
        "        lag_states = self.get_lag_state(x_seq)  # (B, T-tau, D*(tau+1))\n",
        "        encoded_lag = self.lag_encoder(lag_states)  # (B, T-tau, H)\n",
        "\n",
        "        # 2. 初始化SDDE积分：取第一个时滞特征作为初始状态\n",
        "        z0 = encoded_lag[:, 0, :]  # (B, H)\n",
        "        if t_eval is None:\n",
        "            t_eval = torch.linspace(0.0, 1.0, seq_len - self.tau, device=device)\n",
        "\n",
        "        # 3. SDDE数值积分（使用odeint近似随机积分，参考SDDE_net.pdf的Euler-Maruyama离散化）\n",
        "        def drift_func(t, z):  # 仅返回漂移项，用于odeint接口（扩散项用于不确定性分析）\n",
        "            return self.drift_net(z)\n",
        "\n",
        "        z_path = odeint(\n",
        "            func=drift_func,\n",
        "            y0=z0,\n",
        "            t=t_eval,\n",
        "            method='dopri5',  # 高精度数值积分方法\n",
        "            adjoint_params=list(self.drift_net.parameters())\n",
        "        )  # z_path: (T-lag, B, H)\n",
        "        z_path = z_path.transpose(0, 1)  # (B, T-lag, H)\n",
        "\n",
        "        # 4. 计算扩散项（用于不确定性量化，不参与预测）\n",
        "        diffusion = F.softplus(self.diffusion_net(z_path)) + 1e-6\n",
        "\n",
        "        # 5. 解码预测（补齐前tau步的预测值为0，便于统一长度）\n",
        "        pred = self.decoder(z_path)  # (B, T-lag, D)\n",
        "        pred_pad = torch.zeros(batch_size, self.tau, self.input_dim, device=device)  # 前tau步补0\n",
        "        pred_full = torch.cat([pred_pad, pred], dim=1)  # (B, T, D)\n",
        "\n",
        "        return pred_full, diffusion\n",
        "\n",
        "\n",
        "# ============= 3. 对比模型2：MHNN（Hopfield+Transformer） =============\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"轻量级Transformer编码器块（捕捉长程时序依赖）\"\"\"\n",
        "    def __init__(self, dim_model, num_heads=4, dim_feedforward=64):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(dim_model, num_heads, batch_first=True)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(dim_model, dim_feedforward),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, dim_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(dim_model)  # 层归一化，稳定训练\n",
        "        self.norm2 = nn.LayerNorm(dim_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 自注意力层：捕捉长程时序关联\n",
        "        attn_out, _ = self.self_attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)  # 残差+归一化\n",
        "\n",
        "        # 前馈网络层：非线性变换\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = self.norm2(x + ff_out)  # 残差+归一化\n",
        "        return x\n",
        "\n",
        "class MHNN(nn.Module):\n",
        "    \"\"\"MHNN模型：Hopfield记忆层 + Transformer（无随机性建模，纯时序记忆）\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=32, num_heads=4, num_transformer_blocks=2):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # 1. 输入编码器：映射原始序列到隐藏空间\n",
        "        self.input_encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # 2. 核心层：Hopfield记忆（提取关键模式）→ Transformer（长程依赖）\n",
        "        self.hopfield_layer = HopfieldLayer(hidden_dim, num_heads=num_heads)  # 复用Hopfield层\n",
        "        self.transformer_encoder = nn.Sequential(\n",
        "            *[TransformerEncoderBlock(hidden_dim, num_heads) for _ in range(num_transformer_blocks)]\n",
        "        )\n",
        "\n",
        "        # 3. 解码器：从记忆-时序特征映射回输出\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_seq):\n",
        "        \"\"\"x_seq: (B, T, D) → 输出预测序列 (B, T, D)\"\"\"\n",
        "        # 1. 输入编码\n",
        "        encoded = self.input_encoder(x_seq)  # (B, T, H)\n",
        "\n",
        "        # 2. 记忆增强 + 长程依赖捕捉\n",
        "        memory_enhanced = self.hopfield_layer(encoded)  # Hopfield提取关键模式\n",
        "        temporal_enhanced = self.transformer_encoder(memory_enhanced)  # Transformer捕捉长程依赖\n",
        "\n",
        "        # 3. 解码预测\n",
        "        pred = self.decoder(temporal_enhanced)  # (B, T, D)\n",
        "        return pred\n",
        "\n",
        "\n",
        "# ============= 4. 原有模型：Hopfield-SDE（基准模型） =============\n",
        "class HopfieldSDENet(nn.Module):\n",
        "    \"\"\"融合Hopfield记忆与Neural SDE的模型（原有代码，保持一致）\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=32, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # 1. Hopfield记忆层：提取序列历史特征\n",
        "        self.memory_layer = HopfieldLayer(hidden_dim, num_heads=num_heads)\n",
        "\n",
        "        # 2. 编码器：将输入序列映射到隐藏空间\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # 3. Neural SDE 组件：描述隐藏空间动力学\n",
        "        self.drift_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        self.diffusion_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # 4. 解码器：将隐藏空间映射回原始输入空间\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_seq, t_eval=None, return_path=False):\n",
        "        batch_size, seq_len, _ = x_seq.shape\n",
        "        device = x_seq.device\n",
        "\n",
        "        if t_eval is None:\n",
        "            t_eval = torch.linspace(0.0, 1.0, seq_len, device=device)\n",
        "\n",
        "        # 步骤1：编码输入序列到隐藏空间\n",
        "        encoded = self.encoder(x_seq)  # (B, T, H)\n",
        "\n",
        "        # 步骤2：Hopfield记忆层提取历史关键特征\n",
        "        memory = self.memory_layer(encoded)  # (B, T, H)\n",
        "\n",
        "        # 步骤3：取最后时刻的记忆作为SDE初始状态\n",
        "        z0 = memory[:, -1, :]  # (B, H)\n",
        "\n",
        "        # 步骤4：SDE动力学积分\n",
        "        def drift_func(t, z):\n",
        "            return self.drift_net(z)\n",
        "\n",
        "        z_path = odeint(\n",
        "            func=drift_func,\n",
        "            y0=z0,\n",
        "            t=t_eval,\n",
        "            method='dopri5',\n",
        "            adjoint_params=list(self.drift_net.parameters())\n",
        "        )  # (T, B, H)\n",
        "        z_path = z_path.transpose(0, 1)  # (B, T, H)\n",
        "\n",
        "        # 步骤5：计算扩散项（随机性强度）\n",
        "        diffusion = F.softplus(self.diffusion_net(z_path)) + 1e-6\n",
        "\n",
        "        # 步骤6：解码预测\n",
        "        output = self.decoder(z_path)  # (B, T, D)\n",
        "\n",
        "        if return_path:\n",
        "            return output, z_path, diffusion\n",
        "        return output\n",
        "\n",
        "\n",
        "# ============= 5. 数据生成：Lorenz混沌系统（统一数据集） =============\n",
        "def generate_lorenz_data(\n",
        "    num_samples=200,\n",
        "    seq_len=50,\n",
        "    dt=0.01,\n",
        "    sigma=10.0,\n",
        "    rho=28.0,\n",
        "    beta=8/3.0\n",
        "):\n",
        "    \"\"\"生成Lorenz混沌系统数据（带随机性，适配时滞/随机模型对比）\"\"\"\n",
        "    def lorenz_dynamics(state):\n",
        "        x, y, z = state\n",
        "        dx = sigma * (y - x)\n",
        "        dy = x * (rho - z) - y\n",
        "        dz = x * y - beta * z\n",
        "        return np.array([dx, dy, dz])\n",
        "\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        # 随机初始状态（[-10, 10]）\n",
        "        state = np.random.uniform(-10, 10, size=3)\n",
        "        trajectory = [state.copy()]\n",
        "\n",
        "        # 四阶龙格-库塔法（RK4）生成轨迹（含混沌特性）\n",
        "        for _ in range(seq_len - 1):\n",
        "            k1 = lorenz_dynamics(state)\n",
        "            k2 = lorenz_dynamics(state + 0.5 * dt * k1)\n",
        "            k3 = lorenz_dynamics(state + 0.5 * dt * k2)\n",
        "            k4 = lorenz_dynamics(state + dt * k3)\n",
        "            state = state + (dt / 6) * (k1 + 2*k2 + 2*k3 + k4)\n",
        "            # 加入微小噪声（模拟真实场景随机性）\n",
        "            state += np.random.normal(0, 0.01, size=3)\n",
        "            trajectory.append(state.copy())\n",
        "\n",
        "        data.append(trajectory)\n",
        "\n",
        "    return np.array(data, dtype=np.float32)  # (num_samples, seq_len, 3)\n",
        "\n",
        "\n",
        "# ============= 6. 统一训练函数（三种模型复用） =============\n",
        "def train_unified_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=50,\n",
        "    lr=1e-3,\n",
        "    device='cpu',\n",
        "    print_interval=10,\n",
        "    model_name=\"Model\"\n",
        "):\n",
        "    \"\"\"统一训练逻辑，返回训练/验证损失与测试MSE\"\"\"\n",
        "    # 优化器与损失函数（三种模型统一）\n",
        "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)  # L2正则化防过拟合\n",
        "    criterion = nn.MSELoss()  # 时序预测用MSE损失\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # 训练阶段\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # 适配不同模型输出（SDDE-Net返回(pred, diffusion)，其他返回pred）\n",
        "            pred_output = model(batch_x)\n",
        "            batch_pred = pred_output[0] if isinstance(pred_output, tuple) else pred_output\n",
        "\n",
        "            loss = criterion(batch_pred, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_x.size(0)\n",
        "\n",
        "        # 计算平均训练损失\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # 验证阶段（无梯度）\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "                pred_output = model(batch_x)\n",
        "                batch_pred = pred_output[0] if isinstance(pred_output, tuple) else pred_output\n",
        "                loss = criterion(batch_pred, batch_y)\n",
        "                val_loss += loss.item() * batch_x.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # 打印进度\n",
        "        if (epoch + 1) % print_interval == 0:\n",
        "            print(f\"[{model_name}] Epoch [{epoch+1:02d}/{epochs:02d}] \"\n",
        "                  f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    # 测试阶段：计算测试集MSE\n",
        "    test_mse = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in val_loader:  # 此处用验证集代测试集（数据量限制）\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            pred_output = model(batch_x)\n",
        "            batch_pred = pred_output[0] if isinstance(pred_output, tuple) else pred_output\n",
        "            test_mse += criterion(batch_pred, batch_y).item() * batch_x.size(0)\n",
        "    test_mse /= len(val_loader.dataset)\n",
        "\n",
        "    return train_losses, val_losses, test_mse, model\n",
        "\n",
        "\n",
        "# ============= 7. 主程序：三种模型训练+对比可视化 =============\n",
        "if __name__ == '__main__':\n",
        "    # 1. 基础配置（统一超参数，确保公平对比）\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    input_dim = 3  # Lorenz数据维度（x,y,z）\n",
        "    hidden_dim = 32  # 隐藏维度统一\n",
        "    epochs = 50  # 训练轮次统一\n",
        "    lr = 1e-3  # 学习率统一\n",
        "    batch_size = 32  # 批次大小统一\n",
        "    print(f\"使用设备: {device} | 超参数统一：hidden_dim={hidden_dim}, epochs={epochs}, lr={lr}\")\n",
        "\n",
        "    # 2. 生成并预处理数据（三种模型共用同一数据集）\n",
        "    print(\"\\nStep 1: 生成Lorenz混沌数据集...\")\n",
        "    lorenz_data = generate_lorenz_data(num_samples=200, seq_len=50)  # (200, 50, 3)\n",
        "    # 时序预测任务：用前T-1步预测后T-1步（一步预测）\n",
        "    X = lorenz_data[:, :-1, :]  # 输入：(200, 49, 3)\n",
        "    y = lorenz_data[:, 1:, :]   # 标签：(200, 49, 3)\n",
        "\n",
        "    # 数据标准化（避免量级差异影响训练）\n",
        "    X_mean = X.mean()\n",
        "    X_std = X.std()\n",
        "    X = (X - X_mean) / (X_std + 1e-8)\n",
        "    y = (y - X_mean) / (X_std + 1e-8)\n",
        "\n",
        "    # 划分训练集/验证集（8:2）\n",
        "    split_idx = int(0.8 * len(X))\n",
        "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "    # 创建DataLoader（统一加载逻辑）\n",
        "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # 3. 初始化三种模型（超参数完全统一）\n",
        "    print(\"\\nStep 2: 初始化三种对比模型...\")\n",
        "    # 模型1：Hopfield-SDE（基准）\n",
        "    model_hopfield_sde = HopfieldSDENet(\n",
        "        input_dim=input_dim, hidden_dim=hidden_dim, num_heads=4\n",
        "    )\n",
        "    # 模型2：SDDE-Net（带时滞建模）\n",
        "    model_sdde = SDDE_Net(\n",
        "        input_dim=input_dim, hidden_dim=hidden_dim, tau=2  # 时滞步数=2\n",
        "    )\n",
        "    # 模型3：MHNN（Hopfield+Transformer）\n",
        "    model_mhnn = MHNN(\n",
        "        input_dim=input_dim, hidden_dim=hidden_dim, num_heads=4, num_transformer_blocks=2\n",
        "    )\n",
        "\n",
        "    # 打印模型参数量（复杂度对比）\n",
        "    def count_params(model):\n",
        "        return sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Hopfield-SDE参数量: {count_params(model_hopfield_sde):,}\")\n",
        "    print(f\"SDDE-Net参数量: {count_params(model_sdde):,}\")\n",
        "    print(f\"MHNN参数量: {count_params(model_mhnn):,}\")\n",
        "\n",
        "    # 4. 训练三种模型（统一训练逻辑）\n",
        "    print(\"\\nStep 3: 训练三种模型...\")\n",
        "    # 训练Hopfield-SDE\n",
        "    print(\"\\n=== 训练Hopfield-SDE ===\")\n",
        "    losses_hsde_train, losses_hsde_val, mse_hsde, model_hsde_trained = train_unified_model(\n",
        "        model_hopfield_sde, train_loader, val_loader, epochs, lr, device, model_name=\"Hopfield-SDE\"\n",
        "    )\n",
        "    # 训练SDDE-Net\n",
        "    print(\"\\n=== 训练SDDE-Net ===\")\n",
        "    losses_sdde_train, losses_sdde_val, mse_sdde, model_sdde_trained = train_unified_model(\n",
        "        model_sdde, train_loader, val_loader, epochs, lr, device, model_name=\"SDDE-Net\"\n",
        "    )\n",
        "    # 训练MHNN\n",
        "    print(\"\\n=== 训练MHNN ===\")\n",
        "    losses_mhnn_train, losses_mhnn_val, mse_mhnn, model_mhnn_trained = train_unified_model(\n",
        "        model_mhnn, train_loader, val_loader, epochs, lr, device, model_name=\"MHNN\"\n",
        "    )\n",
        "\n",
        "    # 5. 结果可视化对比\n",
        "    print(\"\\nStep 4: 可视化三种模型对比结果...\")\n",
        "    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    models = [\"Hopfield-SDE\", \"SDDE-Net\", \"MHNN\"]\n",
        "    train_losses = [losses_hsde_train, losses_sdde_train, losses_mhnn_train]\n",
        "    val_losses = [losses_hsde_val, losses_sdde_val, losses_mhnn_val]\n",
        "    test_mses = [mse_hsde, mse_sdde, mse_mhnn]\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "\n",
        "    # 子图1：训练/验证损失曲线对比\n",
        "    for i, (model, train_loss, val_loss, color) in enumerate(zip(models, train_losses, val_losses, colors)):\n",
        "        ax1.plot(range(1, epochs+1), train_loss, label=f'{model} (Train)', linewidth=2, color=color)\n",
        "        ax1.plot(range(1, epochs+1), val_loss, label=f'{model} (Val)', linewidth=2, color=color, linestyle='--')\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('MSE Loss', fontsize=12)\n",
        "    ax1.set_title('三种模型训练/验证损失对比', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # 子图2：测试集MSE对比（柱状图）\n",
        "    bars = ax2.bar(models, test_mses, color=colors, alpha=0.7, edgecolor='black')\n",
        "    for bar, mse in zip(bars, test_mses):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1e-5,\n",
        "                 f'{mse:.6f}', ha='center', va='bottom', fontsize=10)\n",
        "    ax2.set_ylabel('测试集MSE', fontsize=12)\n",
        "    ax2.set_title('三种模型预测精度对比（MSE越低越好）', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # 子图3：单样本预测轨迹对比（X维度，恢复原始尺度）\n",
        "    model_hsde_trained.eval()\n",
        "    model_sdde_trained.eval()\n",
        "    model_mhnn_trained.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_idx = 0  # 选择第一条验证样本\n",
        "        x_sample = torch.from_numpy(X_val[sample_idx:sample_idx+1]).to(device)  # (1, 49, 3)\n",
        "        y_true = y_val[sample_idx, :, 0] * X_std + X_mean  # 真实值（原始尺度）\n",
        "\n",
        "        # 三种模型预测\n",
        "        pred_hsde = model_hsde_trained(x_sample)[0, :, 0].cpu().numpy() * X_std + X_mean\n",
        "        pred_sdde = model_sdde_trained(x_sample)[0][0, :, 0].cpu().numpy() * X_std + X_mean\n",
        "        pred_mhnn = model_mhnn_trained(x_sample)[0, :, 0].cpu().numpy() * X_std + X_mean\n",
        "\n",
        "        time_steps = np.arange(len(y_true))\n",
        "        ax3.plot(time_steps, y_true, label='真实值（X维度）', linewidth=3, color='black')\n",
        "        ax3.plot(time_steps, pred_hsde, label='Hopfield-SDE', linewidth=2.5, color=colors[0])\n",
        "        ax3.plot(time_steps, pred_sdde, label='SDDE-Net', linewidth=2.5, color=colors[1])\n",
        "        ax3.plot(time_steps, pred_mhnn, label='MHNN', linewidth=2.5, color=colors[2])\n",
        "        ax3.set_xlabel('时间步', fontsize=12)\n",
        "        ax3.set_ylabel('Lorenz系统X维度值', fontsize=12)\n",
        "        ax3.set_title('单样本预测轨迹对比', fontsize=14, fontweight='bold')\n",
        "        ax3.legend(fontsize=10)\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # 子图4：不确定性量化对比（仅SDDE-Net和Hopfield-SDE支持）\n",
        "    with torch.no_grad():\n",
        "        # 计算两种模型的平均扩散项（不确定性强度）\n",
        "        x_batch = torch.from_numpy(X_val[:5]).to(device)  # 取5个样本统计\n",
        "        # Hopfield-SDE扩散项\n",
        "        _, _, diff_hsde = model_hsde_trained(x_batch, return_path=True)\n",
        "        mean_diff_hsde = diff_hsde.mean(dim=(0,1,2)).cpu().numpy()  # 平均不确定性\n",
        "        # SDDE-Net扩散项\n",
        "        _, diff_sdde = model_sdde_trained(x_batch)\n",
        "        mean_diff_sdde = diff_sdde.mean(dim=(0,1,2)).cpu().numpy()  # 平均不确定性\n",
        "\n",
        "        ax4.bar([\"Hopfield-SDE\", \"SDDE-Net\"], [mean_diff_hsde, mean_diff_sdde],\n",
        "                color=colors[:2], alpha=0.7, edgecolor='black')\n",
        "        ax4.set_ylabel('平均扩散项（不确定性强度）', fontsize=12)\n",
        "        ax4.set_title('不确定性量化对比（MHNN无此功能）', fontsize=14, fontweight='bold')\n",
        "        ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # 保存对比图\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('three_models_comparison_lorenz.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\n对比结果图保存至: three_models_comparison_lorenz.png\")\n",
        "\n",
        "    # 6. 三种模型优劣总结\n",
        "    print(\"\\nStep 5: 三种模型优劣总结\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'模型':<15} {'预测精度（MSE）':<20} {'时滞建模':<15} {'不确定性量化':<15} {'计算复杂度（参数）':<20}\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{models[0]:<15} {test_mses[0]:<20.6f} {'无':<15} {'支持':<15} {count_params(model_hopfield_sde):<20,}\")\n",
        "    print(f\"{models[1]:<15} {test_mses[1]:<20.6f} {'支持（tau=2）':<15} {'支持':<15} {count_params(model_sdde):<20,}\")\n",
        "    print(f\"{models[2]:<15} {test_mses[2]:<20.6f} {'无（靠Transformer近似）':<15} {'不支持':<15} {count_params(model_mhnn):<20,}\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\n训练与对比完成！\")"
      ],
      "metadata": {
        "id": "8_WlunGPtYin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##HNSDDE\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchdiffeq import odeint_adjoint as odeint  # 适配SDDE数值积分\n",
        "\n",
        "\n",
        "# ============ 1. 基础组件：支持时滞特征的Hopfield记忆层 ============\n",
        "class HopfieldLayer(nn.Module):\n",
        "    \"\"\"现代Hopfield层（适配时滞特征：输入为“当前+时滞”联合特征）\"\"\"\n",
        "    def __init__(self, dim_model, num_heads=4, beta=1.0):\n",
        "        super().__init__()\n",
        "        self.dim_model = dim_model  # 输入维度=原特征维度*(时滞步数+1)\n",
        "        self.num_heads = num_heads\n",
        "        self.beta = beta\n",
        "        self.head_dim = dim_model // num_heads\n",
        "        assert dim_model % num_heads == 0, \"dim_model must be divisible by num_heads\"\n",
        "\n",
        "        # Query/Key/Value投影（适配时滞联合特征的记忆检索）\n",
        "        self.query_proj = nn.Linear(dim_model, dim_model)\n",
        "        self.key_proj = nn.Linear(dim_model, dim_model)\n",
        "        self.value_proj = nn.Linear(dim_model, dim_model)\n",
        "        self.output_proj = nn.Linear(dim_model, dim_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x: (batch_size, seq_len, dim_model)，含时滞信息的联合特征\"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # 1. 时滞特征投影到Hopfield空间\n",
        "        Q = self.query_proj(x)\n",
        "        K = self.key_proj(x)\n",
        "        V = self.value_proj(x)\n",
        "\n",
        "        # 2. 多头记忆检索（强化时滞模式的关联匹配）\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # 3. Hopfield记忆更新：聚焦时滞模式的相似度匹配\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
        "        scores = scores * self.beta  # 调节时滞模式的匹配锐度\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # 4. 加权输出时滞增强的记忆特征\n",
        "        memory_output = torch.matmul(attn_weights, V)\n",
        "        memory_output = memory_output.transpose(1, 2).contiguous()\n",
        "        memory_output = memory_output.view(batch_size, seq_len, self.dim_model)\n",
        "        return self.output_proj(memory_output) + x  # 残差保留原始时滞信息\n",
        "\n",
        "\n",
        "# ============ 2. 核心模型：Hopfield-SDDE（Hopfield+神经时滞随机微分方程） ============\n",
        "class HopfieldSDDENet(nn.Module):\n",
        "    \"\"\"\n",
        "    修正核心：显式加入时滞项\n",
        "    - 时滞窗口：提取前τ步状态，与当前状态拼接为联合特征\n",
        "    - SDDE动力学：漂移项/扩散项均依赖Hopfield记忆后的时滞特征\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=32, num_heads=4, tau=2):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tau = tau  # 时滞步数：依赖前τ步状态（核心新增参数）\n",
        "        self.lag_feat_dim = input_dim * (tau + 1)  # 时滞联合特征维度=输入维度*(τ+1)\n",
        "\n",
        "        # 1. 时滞特征编码器：将“当前+τ步时滞”状态映射到隐藏空间\n",
        "        self.lag_encoder = nn.Sequential(\n",
        "            nn.Linear(self.lag_feat_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # 2. Hopfield记忆层：记忆时滞联合特征的关键模式\n",
        "        self.hopfield_memory = HopfieldLayer(hidden_dim, num_heads=num_heads)\n",
        "\n",
        "        # 3. SDDE核心（显式依赖时滞记忆特征）\n",
        "        # 漂移项：带时滞的确定性动力学\n",
        "        self.sdde_drift = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        # 扩散项：带时滞的随机性强度（确保非负）\n",
        "        self.sdde_diffusion = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # 4. 解码器：从SDDE时滞动力学输出映射回原始维度\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "\n",
        "    def extract_lag_features(self, x_seq):\n",
        "        \"\"\"\n",
        "        提取时滞联合特征：x_seq (B, T, D) → 输出 (B, T-τ, D*(τ+1))\n",
        "        - 对每个时刻t，拼接 [x(t-τ), x(t-τ+1), ..., x(t)] 作为时滞窗口\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x_seq.shape\n",
        "        lag_features = []\n",
        "        # 从第τ步开始（确保有足够历史时滞）\n",
        "        for t in range(self.tau, seq_len):\n",
        "            # 时滞窗口：前τ步 + 当前步\n",
        "            lag_window = x_seq[:, t - self.tau : t + 1, :]  # (B, τ+1, D)\n",
        "            lag_window_flat = lag_window.view(batch_size, -1)  # (B, D*(τ+1))\n",
        "            lag_features.append(lag_window_flat)\n",
        "        return torch.stack(lag_features, dim=1)  # (B, T-τ, D*(τ+1))\n",
        "\n",
        "    def sdde_dynamics(self, t, z):\n",
        "        \"\"\"\n",
        "        SDDE动力学方程（显式依赖时滞记忆特征z）\n",
        "        dz = drift(z)dt + diffusion(z)dW(t)\n",
        "        \"\"\"\n",
        "        drift = self.sdde_drift(z)  # 时滞依赖的确定性增量\n",
        "        diffusion = F.softplus(self.sdde_diffusion(z)) + 1e-6  # 时滞依赖的随机强度\n",
        "        return drift, diffusion\n",
        "\n",
        "    def forward(self, x_seq, t_eval=None, return_diffusion=False):\n",
        "        batch_size, seq_len, _ = x_seq.shape\n",
        "        device = x_seq.device\n",
        "\n",
        "        # 1. 步骤1：提取时滞联合特征（核心修正：新增时滞处理）\n",
        "        lag_feat = self.extract_lag_features(x_seq)  # (B, T-τ, D*(τ+1))\n",
        "        # 适配后续序列长度（T' = T-τ）\n",
        "        seq_len_lag = lag_feat.shape[1]\n",
        "\n",
        "        # 2. 步骤2：时滞特征编码 + Hopfield记忆增强\n",
        "        encoded_lag = self.lag_encoder(lag_feat)  # (B, T', H)\n",
        "        memory_lag = self.hopfield_memory(encoded_lag)  # (B, T', H)：含时滞的记忆特征\n",
        "\n",
        "        # 3. 步骤3：SDDE数值积分（初始状态为第一个时滞记忆特征）\n",
        "        z0 = memory_lag[:, 0, :]  # (B, H)：SDDE初始状态（含时滞信息）\n",
        "        if t_eval is None:\n",
        "            # 积分时间点：与记忆特征长度一致\n",
        "            t_eval = torch.linspace(0.0, 1.0, seq_len_lag, device=device)\n",
        "\n",
        "        # SDDE积分（用odeint近似随机积分，适配时滞特征）\n",
        "        def drift_func(t, z):\n",
        "            return self.sdde_drift(z)  # 仅返回漂移项用于积分接口\n",
        "\n",
        "        z_path = odeint(\n",
        "            func=drift_func,\n",
        "            y0=z0,\n",
        "            t=t_eval,\n",
        "            method='dopri5',  # 高精度数值积分\n",
        "            adjoint_params=list(self.sdde_drift.parameters())\n",
        "        )  # z_path: (T', B, H)\n",
        "        z_path = z_path.transpose(0, 1)  # (B, T', H)：时滞记忆特征的动力学演化路径\n",
        "\n",
        "        # 4. 步骤4：计算扩散项（用于不确定性量化）\n",
        "        diffusion = F.softplus(self.sdde_diffusion(z_path)) + 1e-6\n",
        "\n",
        "        # 5. 步骤5：解码预测（补齐前τ步为0，与原始输入长度一致）\n",
        "        pred_lag = self.decoder(z_path)  # (B, T', D)\n",
        "        pred_pad = torch.zeros(batch_size, self.tau, self.input_dim, device=device)  # 前τ步补0\n",
        "        pred_full = torch.cat([pred_pad, pred_lag], dim=1)  # (B, T, D)：与输入长度一致\n",
        "\n",
        "        # 按需返回结果\n",
        "        if return_diffusion:\n",
        "            return pred_full, diffusion\n",
        "        return pred_full\n",
        "\n",
        "\n",
        "# ============ 3. 数据生成：带时滞特性的Lorenz混沌系统（适配SDDE） ============\n",
        "def generate_lorenz_with_lag(\n",
        "    num_samples=200,\n",
        "    seq_len=50,\n",
        "    dt=0.01,\n",
        "    sigma=10.0,\n",
        "    rho=28.0,\n",
        "    beta=8/3.0,\n",
        "    lag_strength=0.2  # 人为增强时滞影响，让数据时滞特性更明显\n",
        "):\n",
        "    \"\"\"生成带显式时滞特性的Lorenz数据（确保SDDE时滞建模有意义）\"\"\"\n",
        "    def lorenz_lag_dynamics(state, state_lag):\n",
        "        \"\"\"Lorenz动力学+时滞依赖：当前状态依赖前1步时滞状态\"\"\"\n",
        "        x, y, z = state\n",
        "        x_lag, y_lag, z_lag = state_lag\n",
        "        # 加入时滞项影响：当前dx依赖前1步的x_lag\n",
        "        dx = sigma * (y - x) + lag_strength * x_lag\n",
        "        dy = x * (rho - z) - y + lag_strength * y_lag\n",
        "        dz = x * y - beta * z + lag_strength * z_lag\n",
        "        return np.array([dx, dy, dz])\n",
        "\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        # 初始状态（前τ=2步，用于构建初始时滞）\n",
        "        state_prev2 = np.random.uniform(-10, 10, size=3)  # t-2步\n",
        "        state_prev1 = np.random.uniform(-10, 10, size=3)  # t-1步（τ=2时的时滞状态）\n",
        "        state = np.random.uniform(-10, 10, size=3)        # 当前步\n",
        "        trajectory = [state_prev2, state_prev1, state]\n",
        "\n",
        "        # 生成带时滞的轨迹（τ=2，依赖前2步状态）\n",
        "        for _ in range(seq_len - 3):\n",
        "            # 时滞状态：前2步\n",
        "            state_lag2 = trajectory[-3]\n",
        "            state_lag1 = trajectory[-2]\n",
        "            # 动力学更新（依赖前2步时滞）\n",
        "            dxdy dz = lorenz_lag_dynamics(trajectory[-1], state_lag1)\n",
        "            state_next = trajectory[-1] + dt * dxdy dz\n",
        "            # 加入随机噪声（符合SDDE的随机特性）\n",
        "            state_next += np.random.normal(0, 0.01, size=3)\n",
        "            trajectory.append(state_next)\n",
        "\n",
        "        # 截取seq_len长度（确保统一维度）\n",
        "        trajectory = trajectory[:seq_len]\n",
        "        data.append(trajectory)\n",
        "\n",
        "    return np.array(data, dtype=np.float32)  # (num_samples, seq_len, 3)\n",
        "\n",
        "\n",
        "# ============ 4. 训练与可视化（适配Hopfield-SDDE） ============\n",
        "def train_hopfield_sdde(model, train_loader, val_loader, epochs=50, lr=1e-3, device='cpu'):\n",
        "    \"\"\"训练Hopfield-SDDE模型（适配时滞特征输入）\"\"\"\n",
        "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    criterion = nn.MSELoss()\n",
        "    train_losses, val_losses = [], []\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # 训练阶段\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 前向传播（Hopfield-SDDE处理时滞特征）\n",
        "            batch_pred = model(batch_x)\n",
        "            # 计算损失（仅对非补齐部分计算，避免前τ步0值干扰）\n",
        "            loss = criterion(batch_pred[:, model.tau:, :], batch_y[:, model.tau:, :])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * batch_x.size(0)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # 验证阶段\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "                batch_pred = model(batch_x)\n",
        "                loss = criterion(batch_pred[:, model.tau:, :], batch_y[:, model.tau:, :])\n",
        "                val_loss += loss.item() * batch_x.size(0)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # 打印进度\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1:02d}/{epochs}] | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    return train_losses, val_losses, model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 1. 设备与超参数（聚焦时滞τ=2）\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    input_dim = 3  # Lorenz数据维度（x,y,z）\n",
        "    hidden_dim = 32\n",
        "    tau = 2  # 时滞步数（核心参数）\n",
        "    epochs = 50\n",
        "    batch_size = 32\n",
        "\n",
        "    # 2. 生成带时滞的Lorenz数据（适配SDDE）\n",
        "    print(\"Step 1: 生成带时滞特性的Lorenz数据...\")\n",
        "    lorenz_data = generate_lorenz_with_lag(num_samples=200, seq_len=50, lag_strength=0.2)\n",
        "    X = lorenz_data[:, :-1, :]  # 输入：(200, 49, 3)\n",
        "    y = lorenz_data[:, 1:, :]   # 标签：(200, 49, 3)\n",
        "\n",
        "    # 数据标准化\n",
        "    X_mean = X.mean()\n",
        "    X_std = X.std()\n",
        "    X = (X - X_mean) / (X_std + 1e-8)\n",
        "    y = (y - X_mean) / (X_std + 1e-8)\n",
        "\n",
        "    # 划分数据集\n",
        "    split_idx = int(0.8 * len(X))\n",
        "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "    # 创建DataLoader\n",
        "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # 3. 初始化并训练Hopfield-SDDE模型\n",
        "    print(\"\\nStep 2: 初始化Hopfield-SDDE模型...\")\n",
        "    model = HopfieldSDDENet(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_heads=4,\n",
        "        tau=tau  # 显式传入时滞步数\n",
        "    )\n",
        "    print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    print(\"\\nStep 3: 训练Hopfield-SDDE模型...\")\n",
        "    train_losses, val_losses, trained_model = train_hopfield_sdde(\n",
        "        model, train_loader, val_loader, epochs=epochs, device=device\n",
        "    )\n",
        "\n",
        "    # 4. 结果可视化（聚焦时滞部分的预测效果）\n",
        "    print(\"\\nStep 4: 可视化结果...\")\n",
        "    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # 子图1：训练/验证损失\n",
        "    ax1.plot(range(1, epochs+1), train_losses, label='Train Loss', color='#1f77b4', linewidth=2)\n",
        "    ax1.plot(range(1, epochs+1), val_losses, label='Val Loss', color='#ff7f0e', linewidth=2)\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('MSE Loss')\n",
        "    ax1.set_title('Hopfield-SDDE Training & Validation Loss', fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(alpha=0.3)\n",
        "\n",
        "    # 子图2：时滞部分预测轨迹（忽略前τ=2步补齐值）\n",
        "    trained_model.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_idx = 0\n",
        "        x_sample = torch.from_numpy(X_val[sample_idx:sample_idx+1]).to(device)\n",
        "        y_true = y_val[sample_idx, tau:, 0] * X_std + X_mean  # 真实值（跳过前τ步）\n",
        "        y_pred = trained_model(x_sample)[0, tau:, 0].cpu().numpy() * X_std + X_mean  # 预测值（跳过前τ步）\n",
        "        time_steps = np.arange(len(y_true)) + tau  # 时间步对齐（从τ开始）\n",
        "\n",
        "        ax2.plot(time_steps, y_true, label='True X (with Lag)', color='black', linewidth=2.5)\n",
        "        ax2.plot(time_steps, y_pred, label='Pred X (Hopfield-SDDE)', color='#d62728', linewidth=2.5)\n",
        "        ax2.set_xlabel('Time Step')\n",
        "        ax2.set_ylabel('Lorenz X Component')\n",
        "        ax2.set_title(f'Hopfield-SDDE Prediction (Lag τ={tau})', fontweight='bold')\n",
        "        ax2.legend()\n",
        "        ax2.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('hopfield_sdde_results.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\n结果保存至: hopfield_sdde_results.png\")\n",
        "    print(\"\\nHopfield-SDDE（时滞随机微分方程）训练完成！\")"
      ],
      "metadata": {
        "id": "mDjf08AOth4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}